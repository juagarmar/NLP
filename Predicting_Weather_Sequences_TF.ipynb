{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZyN+MtgVLyqZ+DTHDhOpa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juagarmar/NLP/blob/main/Predicting_Weather_Sequences_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nNIh4pWB4O_m"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "#from history import plot_history, save_history\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_data():\n",
        "    precip_file = '../content/data.json'\n",
        "\n",
        "    with open(precip_file, 'r') as f:\n",
        "        precip_raw = json.load(f)\n",
        "\n",
        "    precip_data = precip_raw['data']\n",
        "\n",
        "    time = []\n",
        "    precipitation = []\n",
        "\n",
        "    for month in precip_data:\n",
        "        time.append(month)\n",
        "        precipitation.append(float(precip_data[month]['value']))\n",
        "\n",
        "    return np.array(precipitation), time"
      ],
      "metadata": {
        "id": "qLiIhU5_4X00"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(sequence, time, split_time):\n",
        "    main_seq = sequence[:split_time]\n",
        "    main_time = time[:split_time]\n",
        "    extra_seq = sequence[split_time:]\n",
        "    extra_time = time[split_time:]\n",
        "\n",
        "    print(f'Splitting into {len(main_seq)} main examples and {len(extra_seq)} extra examples.')\n",
        "\n",
        "    return main_seq, main_time, extra_seq, extra_time"
      ],
      "metadata": {
        "id": "zWebxuSc4cpo"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wrangle_data(sequence, data_split, examples, batch_size):\n",
        "    examples = examples + 1\n",
        "    seq_expand = tf.expand_dims(sequence, -1)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(seq_expand)\n",
        "    dataset = dataset.window(examples, shift=1, drop_remainder=True)\n",
        "    dataset = dataset.flat_map(lambda b: b.batch(examples))\n",
        "    dataset = dataset.map(lambda x: (x[:-1], x[-1]))\n",
        "    if data_split == 'train':\n",
        "        dataset = dataset.shuffle(10000)\n",
        "    else:\n",
        "        dataset = dataset.cache()\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "22RDGCEl4cso"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn_model():\n",
        "    new_model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.InputLayer((None, 1)),\n",
        "        tf.keras.layers.Conv1D(64, 3, padding='causal', activation='relu'),\n",
        "        tf.keras.layers.LSTM(48),\n",
        "        tf.keras.layers.Dense(36, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "    return compile_model(new_model)"
      ],
      "metadata": {
        "id": "SptGPBEi4cv4"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_sequence(time, sequences, start=0, end=None):\n",
        "    y_min, y_max = (0.0, 1.0)\n",
        "    if len(np.shape(sequences)) == 1:\n",
        "        sequences = [sequences]\n",
        "    time = time[start:end]\n",
        "    plt.figure(figsize=(28, 8))\n",
        "    for sequence in sequences:\n",
        "        y_max = max(np.max(sequence), y_max)\n",
        "        y_min = min(np.min(sequence), y_min)\n",
        "        sequence = sequence[start:end]\n",
        "        plt.plot(time, sequence)\n",
        "    plt.ylim(y_min, y_max)\n",
        "    plt.xlim(np.min(time), np.max(time))"
      ],
      "metadata": {
        "id": "5IyQLDAu4uH4"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compile_model(new_model):\n",
        "    new_model.compile(optimizer='adam', loss='mae', metrics=[tf.metrics.RootMeanSquaredError()])\n",
        "    print(new_model.summary())\n",
        "    return new_model"
      ],
      "metadata": {
        "id": "kYo3y29O4yUH"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, name, history, test_data):\n",
        "    test_loss, test_rmse = model.evaluate(test_data)\n",
        "\n",
        "    # Save model information\n",
        "    save_name = f'models/precip/{name}-{len(history.epoch):02d}-{test_rmse:0.4f}'\n",
        "    model.save(f'{save_name}.h5')\n",
        "\n",
        "    # Save history information\n",
        "    #save_history(history, save_name)"
      ],
      "metadata": {
        "id": "anS1JEVi4y4k"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_predictions(trained_model, predict_sequence, true_values, predict_time, begin=0, end=None):\n",
        "    predictions = trained_model.predict(predict_sequence)\n",
        "    predictions = predictions[:, -1].reshape(len(predictions))\n",
        "    plot_sequence(predict_time, (true_values, predictions), begin, end)\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "fDR_AbBm4-Lf"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  monthly_precip, time_dates = retrieve_data()"
      ],
      "metadata": {
        "id": "IK8VJUZBDGSz"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "/content/data.json"
      ],
      "metadata": {
        "id": "F8gPBXtWE6_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EWn4lrVLE7M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  monthly_precip, time_dates = retrieve_data()\n",
        "  time_steps = list(range(len(time_dates)))\n",
        "  min_precip = np.min(monthly_precip)\n",
        "  max_precip = np.max(monthly_precip)\n",
        "  precip_norm = (monthly_precip - min_precip) / (max_precip - min_precip)\n",
        "\n",
        "  test_split = time_dates.index('200001')\n",
        "  valid_split = time_dates.index('199001')\n",
        "\n",
        "  train_valid_sp, train_valid_time, test_sp, test_time = split_data(precip_norm, time_steps, test_split)\n",
        "  train_sp, train_time, valid_sp, valid_time = split_data(train_valid_sp, train_valid_time, valid_split)\n",
        "\n",
        "  examples = 6\n",
        "  batch_size = 16\n",
        "\n",
        "  train_data = wrangle_data(train_sp, 'train', examples, batch_size)\n",
        "  valid_data = wrangle_data(valid_sp, 'valid', examples, batch_size)\n",
        "  test_data = wrangle_data(test_sp, 'test', examples, batch_size)\n",
        "\n",
        "  model_name = 'rnn'\n",
        "\n",
        "  earlystop = EarlyStopping('val_loss', patience=15, restore_best_weights=True)\n",
        "  checkpoint = ModelCheckpoint(filepath=f'ckpts/precip/{model_name}/' + '{epoch:02d}-{val_loss:.4f}')\n",
        "\n",
        "  model = rnn_model()\n",
        "\n",
        "  history = model.fit(train_data, epochs=100, validation_data=valid_data,\n",
        "                    callbacks=[earlystop, checkpoint])\n",
        "\n",
        "  #save_model(model, model_name, history, test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNiNfCgG4akn",
        "outputId": "7a7b8eab-1ca5-4911-8b05-5f11e95f55af"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting into 1200 main examples and 252 extra examples.\n",
            "Splitting into 1080 main examples and 120 extra examples.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, None, 64)          256       \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 48)                21696     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 36)                1764      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 37        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23753 (92.79 KB)\n",
            "Trainable params: 23753 (92.79 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "68/68 [==============================] - 7s 70ms/step - loss: 0.1679 - root_mean_squared_error: 0.2213 - val_loss: 0.1082 - val_root_mean_squared_error: 0.1328\n",
            "Epoch 2/100\n",
            "68/68 [==============================] - 5s 68ms/step - loss: 0.1160 - root_mean_squared_error: 0.1451 - val_loss: 0.1077 - val_root_mean_squared_error: 0.1385\n",
            "Epoch 3/100\n",
            "68/68 [==============================] - 4s 64ms/step - loss: 0.1138 - root_mean_squared_error: 0.1423 - val_loss: 0.0996 - val_root_mean_squared_error: 0.1250\n",
            "Epoch 4/100\n",
            "68/68 [==============================] - 4s 56ms/step - loss: 0.1129 - root_mean_squared_error: 0.1410 - val_loss: 0.1031 - val_root_mean_squared_error: 0.1320\n",
            "Epoch 5/100\n",
            "68/68 [==============================] - 5s 69ms/step - loss: 0.1118 - root_mean_squared_error: 0.1395 - val_loss: 0.0982 - val_root_mean_squared_error: 0.1254\n",
            "Epoch 6/100\n",
            "68/68 [==============================] - 4s 65ms/step - loss: 0.1092 - root_mean_squared_error: 0.1374 - val_loss: 0.1001 - val_root_mean_squared_error: 0.1274\n",
            "Epoch 7/100\n",
            "68/68 [==============================] - 4s 63ms/step - loss: 0.1107 - root_mean_squared_error: 0.1388 - val_loss: 0.1208 - val_root_mean_squared_error: 0.1516\n",
            "Epoch 8/100\n",
            "68/68 [==============================] - 4s 56ms/step - loss: 0.1086 - root_mean_squared_error: 0.1374 - val_loss: 0.1037 - val_root_mean_squared_error: 0.1262\n",
            "Epoch 9/100\n",
            "68/68 [==============================] - 5s 66ms/step - loss: 0.1111 - root_mean_squared_error: 0.1398 - val_loss: 0.0957 - val_root_mean_squared_error: 0.1205\n",
            "Epoch 10/100\n",
            "68/68 [==============================] - 5s 69ms/step - loss: 0.1090 - root_mean_squared_error: 0.1378 - val_loss: 0.1117 - val_root_mean_squared_error: 0.1424\n",
            "Epoch 11/100\n",
            "68/68 [==============================] - 4s 56ms/step - loss: 0.1090 - root_mean_squared_error: 0.1377 - val_loss: 0.0977 - val_root_mean_squared_error: 0.1213\n",
            "Epoch 12/100\n",
            "68/68 [==============================] - 5s 71ms/step - loss: 0.1119 - root_mean_squared_error: 0.1392 - val_loss: 0.0955 - val_root_mean_squared_error: 0.1213\n",
            "Epoch 13/100\n",
            "68/68 [==============================] - 4s 60ms/step - loss: 0.1085 - root_mean_squared_error: 0.1368 - val_loss: 0.0962 - val_root_mean_squared_error: 0.1224\n",
            "Epoch 14/100\n",
            "68/68 [==============================] - 4s 63ms/step - loss: 0.1084 - root_mean_squared_error: 0.1372 - val_loss: 0.1170 - val_root_mean_squared_error: 0.1480\n",
            "Epoch 15/100\n",
            "68/68 [==============================] - 5s 69ms/step - loss: 0.1080 - root_mean_squared_error: 0.1361 - val_loss: 0.0957 - val_root_mean_squared_error: 0.1213\n",
            "Epoch 16/100\n",
            "68/68 [==============================] - 4s 54ms/step - loss: 0.1098 - root_mean_squared_error: 0.1381 - val_loss: 0.1007 - val_root_mean_squared_error: 0.1234\n",
            "Epoch 17/100\n",
            "68/68 [==============================] - 4s 64ms/step - loss: 0.1101 - root_mean_squared_error: 0.1381 - val_loss: 0.1289 - val_root_mean_squared_error: 0.1594\n",
            "Epoch 18/100\n",
            "68/68 [==============================] - 5s 69ms/step - loss: 0.1124 - root_mean_squared_error: 0.1413 - val_loss: 0.0990 - val_root_mean_squared_error: 0.1258\n",
            "Epoch 19/100\n",
            "68/68 [==============================] - 4s 62ms/step - loss: 0.1096 - root_mean_squared_error: 0.1375 - val_loss: 0.1157 - val_root_mean_squared_error: 0.1466\n",
            "Epoch 20/100\n",
            "68/68 [==============================] - 4s 57ms/step - loss: 0.1093 - root_mean_squared_error: 0.1372 - val_loss: 0.0960 - val_root_mean_squared_error: 0.1215\n",
            "Epoch 21/100\n",
            "68/68 [==============================] - 5s 71ms/step - loss: 0.1081 - root_mean_squared_error: 0.1360 - val_loss: 0.0957 - val_root_mean_squared_error: 0.1213\n",
            "Epoch 22/100\n",
            "68/68 [==============================] - 5s 66ms/step - loss: 0.1081 - root_mean_squared_error: 0.1365 - val_loss: 0.0990 - val_root_mean_squared_error: 0.1259\n",
            "Epoch 23/100\n",
            "68/68 [==============================] - 4s 61ms/step - loss: 0.1088 - root_mean_squared_error: 0.1371 - val_loss: 0.0982 - val_root_mean_squared_error: 0.1215\n",
            "Epoch 24/100\n",
            "68/68 [==============================] - 4s 63ms/step - loss: 0.1085 - root_mean_squared_error: 0.1379 - val_loss: 0.1297 - val_root_mean_squared_error: 0.1603\n",
            "Epoch 25/100\n",
            "68/68 [==============================] - 5s 68ms/step - loss: 0.1110 - root_mean_squared_error: 0.1394 - val_loss: 0.0985 - val_root_mean_squared_error: 0.1251\n",
            "Epoch 26/100\n",
            "68/68 [==============================] - 5s 71ms/step - loss: 0.1088 - root_mean_squared_error: 0.1371 - val_loss: 0.1035 - val_root_mean_squared_error: 0.1324\n",
            "Epoch 27/100\n",
            "68/68 [==============================] - 4s 65ms/step - loss: 0.1091 - root_mean_squared_error: 0.1374 - val_loss: 0.1008 - val_root_mean_squared_error: 0.1288\n"
          ]
        }
      ]
    }
  ]
}